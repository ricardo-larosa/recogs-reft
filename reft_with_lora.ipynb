{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2fcc080-fd76-455e-82d8-2255f3043d88",
   "metadata": {},
   "source": [
    "### ReFT is complimentary to existing LoRAs (or other PEFTs)\n",
    "\n",
    "You can wrap a PEFT model as a ReFT model and take advantages of both worlds, as light-weight LoRA has no inference overhead while might provide performance gains. Note that LoRA is coupled with model weights, yet it can also been seens as an intervention which edits the original representation.\n",
    "\n",
    "It's very easy to combine PEFTs with ReFT by using existing library such as `peft`:\n",
    "\n",
    "```py\n",
    "import pyreft\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(...)\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "reft_config = pyreft.ReftConfig(...)\n",
    "reft_model = pyreft.get_reft_model(model, reft_config)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4221ca53-0c2d-4288-b2fd-cace955d48a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/nlp/anaconda/main/anaconda3/envs/wuzhengx-310/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # This library is our indicator that the required installs\n",
    "    # need to be done.\n",
    "    import peft\n",
    "\n",
    "except ModuleNotFoundError:\n",
    "    !pip install peft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31814f4-b9ce-46c4-a77d-b1d0b0c11ae3",
   "metadata": {},
   "source": [
    "### Loading our LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c29576a9-bd64-4d79-a033-4255e4b21553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a414df1fe7d4418916fdcf715aa3d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8767c5b38b74838a088430739413c1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "import torch, transformers\n",
    "\n",
    "import pyreft\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "model_name_or_path = \"unsloth/llama-3-8b-bnb-4bit\"\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path, torch_dtype=torch.bfloat16, device_map=\"cuda\")\n",
    "\n",
    "# get tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name_or_path, model_max_length=2048, adding_side=\"right\", use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f870b16",
   "metadata": {},
   "source": [
    "## Prepare ReCOGS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e4acd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee21a440",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import wget\n",
    "\n",
    "SRC_DIRNAME = os.path.join(\"data\", \"recogs\")\n",
    "if not os.path.exists(SRC_DIRNAME):\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    wget.download('https://web.stanford.edu/class/cs224u/data/recogs.tgz', out='data/')\n",
    "    !tar xvf data/recogs.tgz -C data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff75d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import wget\n",
    "\n",
    "SRC_DIRNAME = os.path.join(\"data\", \"recogs\")\n",
    "if not os.path.exists(SRC_DIRNAME):\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    wget.download('https://web.stanford.edu/class/cs224u/data/recogs.tgz', out='data/')\n",
    "    !tar xvf data/recogs.tgz -C data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8283e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def load_split(filename):\n",
    "    return pd.read_csv(\n",
    "        filename,\n",
    "        delimiter=\"\\t\",\n",
    "        names=['input', 'output', 'category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8486d47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {}\n",
    "\n",
    "for splitname in (\"train\", \"dev\", \"gen\"):\n",
    "    dataset[splitname] = load_split(f\"{SRC_DIRNAME}/{splitname}.tsv\")\n",
    "\n",
    "# Data prep\n",
    "from datasets import Dataset\n",
    "trainset = dataset['train']\n",
    "trainset = Dataset.from_pandas(trainset) # convert from pandas to pyarrow format\n",
    "trainset = trainset.remove_columns('category') # remove the category column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfca072",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b8a490-032c-4888-b2dd-649ec5a7e3b5",
   "metadata": {},
   "source": [
    "### That's it! Wrap the LM as a `peft` LM, then as a `pyreft` LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfec23c5-18e7-4583-9df6-7e987b41705d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable intervention params: 32,772 || trainable model params: 32,768\n",
      "model params: 6,738,448,384 || trainable%: 0.0009726274694871952\n"
     ]
    }
   ],
   "source": [
    "include_peft = True\n",
    "layers_to_transform = [15]\n",
    "\n",
    "if include_peft:\n",
    "    peft_config = LoraConfig(\n",
    "        r=4,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"o_proj\"],\n",
    "        layers_to_transform=layers_to_transform,\n",
    "        use_rslora=True,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "reft_config = pyreft.ReftConfig(representations=[{\n",
    "    \"layer\": l, \"component\": f\"base_model.model.model.layers[{l}].output\" if include_peft else \"block_output\",\n",
    "    \"low_rank_dimension\": 4,\n",
    "    \"intervention\": pyreft.LoreftIntervention(embed_dim=model.config.hidden_size,\n",
    "    low_rank_dimension=4)} for l in layers_to_transform])\n",
    "\n",
    "reft_model = pyreft.get_reft_model(model, reft_config)\n",
    "# you need to call this to re-enable lora grads!\n",
    "reft_model.model.enable_adapter_layers()\n",
    "reft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbd4238-97ff-44f8-b2ad-351f6128842e",
   "metadata": {},
   "source": [
    "# Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46f47c05-77af-4309-8f6c-e2ab8b94c46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_no_input_template = \"\"\"<s>[INST] <<SYS>>\n",
    "You are a helpful assistant.\n",
    "<</SYS>>\n",
    "\n",
    "%s [/INST]\n",
    "\"\"\"\n",
    "\n",
    "data_module = pyreft.make_last_position_supervised_data_module(\n",
    "    tokenizer, model, [prompt_no_input_template % e[0] for e in dataset['train']], \n",
    "    [e[1] for e in dataset['train']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13d26c1a-2ff2-4b85-a00c-35591271f553",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/nlp/anaconda/main/anaconda3/envs/wuzhengx-310/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:42, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.692300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.066300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.016300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.015700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train\n",
    "training_args = transformers.TrainingArguments(\n",
    "    num_train_epochs=100.0, output_dir=\"./tmp\", \n",
    "    per_device_train_batch_size=10, \n",
    "    learning_rate=4e-3, \n",
    "    logging_steps=20, \n",
    "    report_to=[])\n",
    "trainer = pyreft.ReftTrainerForCausalLM(\n",
    "    model=reft_model, \n",
    "    tokenizer=tokenizer, \n",
    "    args=training_args, \n",
    "    **data_module)\n",
    "_ = trainer.train()\n",
    "\n",
    "# ensure everything is in eval mode\n",
    "reft_model.model.eval()\n",
    "for k,v in reft_model.interventions.items():\n",
    "    _ = v[0].eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6a7d11-963a-4a36-ad8d-3417ec15012c",
   "metadata": {},
   "source": [
    "**Note**: `loss` looks a bit different if you compare these with the ones in the original ReFT-only training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b88279e4-c55d-42dc-b0b3-71901bd314c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] <<SYS>>\n",
      "You are a helpful assistant.\n",
      "<</SYS>>\n",
      "\n",
      "Which dog breed do people think is cuter, poodle or doodle? [/INST]\n",
      "ðŸ¶ðŸ’¬ðŸ‘€ðŸŒŸ\n"
     ]
    }
   ],
   "source": [
    "instruction = \"Which dog breed do people think is cuter, poodle or doodle?\"\n",
    "\n",
    "# tokenize and prepare the input\n",
    "prompt = prompt_no_input_template % instruction\n",
    "prompt = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "base_unit_location = prompt[\"input_ids\"].shape[-1] - 1  # last position\n",
    "_, reft_response = reft_model.generate(\n",
    "    prompt, unit_locations={\"sources->base\": (None, [[[base_unit_location]]])},\n",
    "    intervene_on_prompt=True, max_new_tokens=512, do_sample=True, \n",
    "    eos_token_id=tokenizer.eos_token_id, early_stopping=True\n",
    ")\n",
    "print(tokenizer.decode(reft_response[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
